{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIAD Active learning pipeline\n",
    "\n",
    "The purpose of this tutorial is to provide a tutorial about the essential functions of NAIAD. We introduce:\n",
    "1) Active Learning pipeline for iterative selection of \"best\" perturbations for model improvement (e.g. for lab-in-the-loop experimental designs)\n",
    "2) Active Learning pipeline with replicates, leveraging ensembling methods to choose the \"best\" perturbations with higher confidence in iterative data collection and training rounds.\n",
    "\n",
    "We will use the [Norman2019](https://www.science.org/doi/10.1126/science.aax4438) combinatorial perturbation dataset, which contains cell viability measurements for CRISPRa pairwise genetic perturbations of ~120 genes.\n",
    "\n",
    "Besides the Norman2019 dataset, you can also try out these models on the [Simpson2023](https://www.biorxiv.org/content/10.1101/2023.08.19.553986v1) and [Horlbeck2018](https://www.cell.com/cell/fulltext/S0092-8674\\(18\\)30735-9) datasets, present within the `./data` subfolder of this `tutorials` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "from naiad import load_naiad_data, NAIAD, ActiveLearner, ActiveLearnerReplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some configuration settings for the notebook\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide path to relevant data directories\n",
    "data_file = './data/norman_gamma.csv'\n",
    "result_dir = './results/active_learning'\n",
    "\n",
    "# parameters for initializing and running model\n",
    "device = (\n",
    "    'cuda' if torch.cuda.is_available() else\n",
    "    'mps' if torch.backends.mps.is_available() else\n",
    "    'cpu'\n",
    ")\n",
    "seed = 42\n",
    "batch_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active learning pipeline arguments\n",
    "n_round = 5\n",
    "n_sample = [100, 200, 300, 400, 500] # number of samples to use in each learning round\n",
    "test_frac = 0.3\n",
    "\n",
    "n_rep = 3          # how many replicates to run of active learning pipeline?\n",
    "n_ensemble = 3     # how many ensembles to use for each round?\n",
    "n_epoch = 10       # how long to train each instance of the model\n",
    "\n",
    "method = 'mean'    # active learning method\n",
    "method_min = True  # method_min checks if we should minimize the value (for 'mean' it is set to True since we are minimizing cell viability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiad_data = load_naiad_data(data_file)\n",
    "active_learner = ActiveLearner(n_round = n_round, \n",
    "                               data = naiad_data,\n",
    "                               n_ensemble = n_ensemble,\n",
    "                               n_epoch = n_epoch,\n",
    "                               n_sample = n_sample, \n",
    "                               test_frac = test_frac, \n",
    "                               early_stop = False,\n",
    "                               device = device,\n",
    "                               batch_size = batch_size,\n",
    "                               method = method, method_min = method_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run active learning pipeline\n",
    "active_learner.set_seed(seed)\n",
    "active_learner.shuffle_data()\n",
    "active_learner.run_active_learning()\n",
    "active_learner.calculate_aggregate_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a few plots to confirm that pipeline ran correctly\n",
    "fig = active_learner.plot_loss_curves(round=2, sampling_type='active', ensemble=0)\n",
    "fig = active_learner.plot_preds(round=2, sampling_type='active', split='test', ensemble=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract final predictions from model\n",
    "preds = active_learner.preds\n",
    "final_round_preds = preds[n_round-1]['active']['overall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning Pipeline with Replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide path to relevant data directories\n",
    "data_source = 'simpson'\n",
    "data_file = './data/simpson_gamma.csv'\n",
    "\n",
    "time_label = datetime.now().strftime(r'%Y-%m-%d-%H-%M-%S')\n",
    "result_dir = os.path.join('./results/active_learning/', f'{data_source}_{time_label}')\n",
    "os.makedirs(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize active learner using same arguments as before\n",
    "naiad_data = load_naiad_data(data_file)\n",
    "active_learner = ActiveLearner(n_round = n_round, \n",
    "                               data = naiad_data,\n",
    "                               n_ensemble = n_ensemble,\n",
    "                               n_epoch = n_epoch,\n",
    "                               n_sample = n_sample, \n",
    "                               test_frac = test_frac, \n",
    "                               early_stop = False,\n",
    "                               device = device,\n",
    "                               batch_size = batch_size,\n",
    "                               method = method, method_min = method_min)\n",
    "\n",
    "active_learner_reps = ActiveLearnerReplicates(n_rep=n_rep, \n",
    "                                              overall_seed=seed, \n",
    "                                              active_learner=active_learner, \n",
    "                                              save_dir=result_dir, save_prefix=data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run active learning with MPE (maximum predicted effect) selection\n",
    "active_learner_reps.set_method(method='mean', method_min=True)          # method_min checks if we should minimize the value (for 'mean' it is set to True since we are minimizing cell viability)\n",
    "results = active_learner_reps.run_replicates(parallel=True)             # parallel distributes the computation for faster processing\n",
    "aggregated_results = active_learner_reps.aggregate_replicate_metrics(return_value=True)\n",
    "active_learner_reps.save_aggregated_results(data_source)                # predictions for each round and replicate are stored in result_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some plots to visualize the active learning results. Since we selected new points to add to the \n",
    "active learning cycle using `method='mean'`, we must specify this same `method` for vusalizing the results.\n",
    "\n",
    "We will visualize the results for both the MSE loss and the TPR for the mean-selection models and the random-selection models.\n",
    "\n",
    "The `train` split shows performance on the training data, the `val` split shows performance on the validation data, and the `overall` split shows performance on both the training and actively-selected points.\n",
    "\n",
    "In order to improve the performance of the active learning workflow, increase `n_epoch` in the ActiveLearner object, and `n_rep` in the ActiveLearnerReplicates object. For processing the Norman dataset in our accompanying publication, we used `n_epoch = 400` with `n_rep = 3`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learner_reps.plot_aggregated_results(metrics = ['mse', 'tpr'], \n",
    "                                            splits = ['train', 'overall'],\n",
    "                                            methods = 'mean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naiad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

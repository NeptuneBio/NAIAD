{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from naiad.models import EmbedPhenoDataset, EmbedPhenoModel, RecoverModel\n",
    "from naiad.utils import load_naiad_data, \\\n",
    "                        split_data, \\\n",
    "                        reorganize_single_treatment_effects, \\\n",
    "                        create_lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    'cuda' if torch.cuda.is_available() else\n",
    "    'mps' if torch.backends.mps.is_available() else\n",
    "    'cpu'\n",
    ")\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = 'norman' # choose one of: ['norman', 'simpson', 'horlbeck_jurkat', 'horlbeck_k562']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (\n",
    "    '../../data/norman/norman_gamma.csv' if data_source == 'norman' else\n",
    "    '../../data/simpson/simpson_gamma.csv' if data_source == 'simpson' else\n",
    "    '../../data/horlbeck/horlbeck_jurkat_gamma.csv' if data_source == 'horlbeck_jurkat' else\n",
    "    '../../data/horlbeck/horlbeck_k562_gamma.csv'\n",
    ")\n",
    "\n",
    "os.mkdir('./results') if not os.path.exists('./results') else None\n",
    "\n",
    "pheno_pert_data = load_naiad_data(data_path)\n",
    "\n",
    "gene_cols = [col for col in pheno_pert_data.columns if re.match(r'id\\d$', col)]\n",
    "all_genes = np.sort(np.unique(pheno_pert_data[gene_cols].values.flatten())).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(data, batch_size, genes):\n",
    "    datasets = {split: EmbedPhenoDataset(data[split], genes) for split in data}\n",
    "    dataloaders = {split: DataLoader(datasets[split], shuffle = False, batch_size = batch_size) for split in datasets}\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_source == 'norman':\n",
    "    d_both_pheno = 64\n",
    "    n_epoch = 500\n",
    "    n_gene = 112\n",
    "    n_test = 500\n",
    "elif data_source == 'simpson':\n",
    "    d_both_pheno = 256\n",
    "    n_epoch = 200\n",
    "    n_gene = 543\n",
    "    n_test = 10000\n",
    "elif 'horlbeck_jurkat' in data_source:\n",
    "    d_both_pheno = 256\n",
    "    n_epoch = 200\n",
    "    n_gene = 437\n",
    "    n_test = 10000\n",
    "elif 'horlbeck_k562' in data_source:\n",
    "    d_both_pheno = 256\n",
    "    n_epoch = 200\n",
    "    n_gene = 448\n",
    "    n_test = 10000\n",
    "\n",
    "train_both_embed = {n_gene: 2,\n",
    "                    n_gene*2: 4,\n",
    "                    n_gene*4: 16,\n",
    "                    n_gene*10: 32,\n",
    "                    n_gene*15: 32,\n",
    "                    n_gene*20: 64,\n",
    "                    n_gene*30: 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = pd.DataFrame()#columns = ['model_type', 'train_loss', 'val_loss',  'train_corr', 'val_corr', 'train_tpr', 'val_tpr', 'test_tpr', 'd_embed', 'n_train', 'seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = {'embed_lr': 1e-2,\n",
    "              'pheno_lr': 1e-2,\n",
    "              'weight_decay': 0,\n",
    "              'n_epoch': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(all_genes, train_dataloader, model_args, train_args):\n",
    "    model_type = model_args['model_type']\n",
    "    weight_decay = train_args['weight_decay']\n",
    "\n",
    "    n_genes = len(all_genes)\n",
    "    if model_type == 'recover':\n",
    "        recover_args = copy.deepcopy(model_args)\n",
    "        recover_args.pop('model_type')\n",
    "        model = RecoverModel(n_genes, **recover_args).to(device)\n",
    "    else:\n",
    "        model = EmbedPhenoModel(n_genes, **model_args).to(device)\n",
    "\n",
    "    if model_type == 'pheno':\n",
    "        pheno_lr = train_args['pheno_lr']\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = pheno_lr, weight_decay = weight_decay)\n",
    "        \n",
    "    elif model_type == 'embed':\n",
    "        embed_lr = train_args['embed_lr']\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = embed_lr, weight_decay = weight_decay)\n",
    "\n",
    "    elif model_type == 'both':\n",
    "        embed_lr = train_args['embed_lr']\n",
    "        pheno_lr = train_args['pheno_lr']\n",
    "        pheno_param = model.pheno_ffn.parameters()\n",
    "        embed_param = [p for p in model.parameters() if p not in set(pheno_param)]\n",
    "        optimizer = torch.optim.Adam([{'params': pheno_param, 'lr': pheno_lr, 'weight_decay': weight_decay}, \n",
    "                                      {'params': embed_param, 'lr': embed_lr, 'weight_decay': weight_decay}])\n",
    "    \n",
    "    elif model_type == 'recover':\n",
    "        embed_lr = train_args['embed_lr']\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = embed_lr, weight_decay = weight_decay)\n",
    "\n",
    "    n_epoch = train_args['n_epoch']\n",
    "    n_train_steps = n_epoch * len(train_dataloader)\n",
    "    lr_scheduler = create_lr_scheduler(optimizer, warmup_steps = int(n_train_steps / 10), total_steps = n_train_steps)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    return model, optimizer, lr_scheduler, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, lr_scheduler, loss_fn, n_epoch):\n",
    "    train_dataloader = dataloaders['train']\n",
    "    val_dataloader = dataloaders['val']\n",
    "    \n",
    "    all_train_loss = []\n",
    "    all_val_loss = []\n",
    "    min_val_loss = np.inf\n",
    "    for i in tqdm(range(n_epoch)):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets, phenos in train_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            phenos = phenos.to(device)\n",
    "\n",
    "            preds = model(inputs, phenos)\n",
    "            loss = loss_fn(targets, preds)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            train_loss += loss\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        for inputs, targets, phenos in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            phenos = phenos.to(device)\n",
    "\n",
    "            preds = model(inputs, phenos)\n",
    "            loss = loss_fn(targets, preds)\n",
    "\n",
    "            val_loss += loss\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "        all_train_loss.append(train_loss.detach().cpu() / dataloaders['train'].dataset.data.shape[0])\n",
    "        all_val_loss.append(val_loss.detach().cpu() / dataloaders['val'].dataset.data.shape[0])\n",
    "        \n",
    "    return model, best_model, all_train_loss, all_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, dataloaders):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = {}\n",
    "    for split in dataloaders:\n",
    "        dataloader = dataloaders[split]\n",
    "        split_preds = []\n",
    "        for input, targets, phenos in dataloader:\n",
    "            input = input.to(device)\n",
    "            targets = targets.to(device)\n",
    "            phenos = phenos.to(device)\n",
    "\n",
    "            preds = model(input, phenos)\n",
    "            split_preds.extend(preds.detach().cpu().tolist())\n",
    "\n",
    "        all_preds[split] = split_preds\n",
    "\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(all_train_loss, all_val_loss, train_targets, val_targets, train_preds, val_preds, alpha=1):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize = [14, 4])\n",
    "    sns.lineplot(np.log(all_train_loss), label = 'Train Loss', ax = axs[0])\n",
    "    sns.lineplot(np.log(all_val_loss), label = 'Val Loss', ax = axs[0])\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('log(Loss)')\n",
    "    axs[0].set_title('Loss Curves')\n",
    "\n",
    "    sns.scatterplot(x = train_targets, y = train_preds, ax = axs[1], alpha=alpha)\n",
    "    axs[1].set_xlabel('Targets')\n",
    "    axs[1].set_ylabel('Preds')\n",
    "    axs[1].set_title('Train Targets vs Preds')\n",
    "\n",
    "    sns.scatterplot(x = val_targets, y = val_preds, ax = axs[2], alpha=alpha)\n",
    "    axs[2].set_xlabel('Targets')\n",
    "    axs[2].set_ylabel('Preds')\n",
    "    axs[2].set_title('Val Targets vs Preds')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tpr(targets, preds, frac):\n",
    "    # assume targets and preds are in same order in terms of perturbation values\n",
    "    if len(targets) != len(preds):\n",
    "        raise ValueError('\"targets\" and \"preds\" arguments should have same length.')\n",
    "    if frac < 1:\n",
    "        n_top = int(frac * len(targets))\n",
    "    else:\n",
    "        n_top = frac\n",
    "    # target_sort_idx = list(reversed(np.argsort(targets).values))[:n_top]\n",
    "    # pred_sort_idx = list(reversed(np.argsort(preds)))[:n_top]\n",
    "    target_sort_idx = list((np.argsort(targets).values))[:n_top]\n",
    "    pred_sort_idx = list((np.argsort(preds)))[:n_top]\n",
    "\n",
    "    return len(set(target_sort_idx).intersection(set(pred_sort_idx))) / n_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_results(results_dict, n_tpr):\n",
    "    results = {'train_loss': [results_dict['train']['loss']], \n",
    "               'val_loss': [results_dict['val']['loss']], \n",
    "               'test_loss': [results_dict['test']['loss']],\n",
    "               'train_corr': [results_dict['train']['corr']], \n",
    "               'val_corr': [results_dict['val']['corr']], \n",
    "               'test_corr': [results_dict['test']['corr']]}\n",
    "    \n",
    "    for split in results_dict:\n",
    "        for tpr in n_tpr:\n",
    "            results[f'{split}_tpr{tpr}'] = [results_dict[split][f'tpr{tpr}']]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:05<00:00, 93.94it/s] \n",
      "100%|██████████| 500/500 [00:04<00:00, 109.12it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]/home/paperspace/Documents/projects/NAIAD/naiad/models.py:469: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3697.)\n",
      "  x0 = self.bilinear_weights.matmul(x[:, 0, :].T).T\n",
      "100%|██████████| 500/500 [00:05<00:00, 91.82it/s] \n",
      "100%|██████████| 500/500 [00:05<00:00, 89.09it/s] \n",
      "100%|██████████| 500/500 [00:05<00:00, 95.58it/s] \n",
      "100%|██████████| 500/500 [00:05<00:00, 87.33it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 81.34it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 83.09it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 73.84it/s]\n",
      "100%|██████████| 500/500 [00:10<00:00, 47.85it/s]\n",
      "100%|██████████| 500/500 [00:09<00:00, 50.33it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 38.82it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 38.73it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 40.73it/s]\n",
      "100%|██████████| 500/500 [00:13<00:00, 36.95it/s]\n",
      "100%|██████████| 500/500 [00:16<00:00, 30.31it/s]\n",
      "100%|██████████| 500/500 [00:15<00:00, 31.93it/s]\n",
      "100%|██████████| 500/500 [00:18<00:00, 27.50it/s]\n",
      "100%|██████████| 500/500 [00:22<00:00, 22.42it/s]\n",
      "100%|██████████| 500/500 [00:21<00:00, 23.21it/s]\n",
      "100%|██████████| 500/500 [00:24<00:00, 20.65it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 101.37it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 108.26it/s]\n",
      "100%|██████████| 500/500 [00:05<00:00, 92.95it/s] \n",
      "100%|██████████| 500/500 [00:05<00:00, 88.49it/s]\n",
      "100%|██████████| 500/500 [00:05<00:00, 96.12it/s] \n",
      "100%|██████████| 500/500 [00:06<00:00, 82.66it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 80.31it/s]\n",
      "100%|██████████| 500/500 [00:05<00:00, 85.25it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 74.90it/s]\n",
      "100%|██████████| 500/500 [00:10<00:00, 49.69it/s]\n",
      "100%|██████████| 500/500 [00:09<00:00, 50.77it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 38.48it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 39.21it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 40.05it/s]\n",
      "100%|██████████| 500/500 [00:13<00:00, 36.47it/s]\n",
      "100%|██████████| 500/500 [00:16<00:00, 30.59it/s]\n",
      "100%|██████████| 500/500 [00:15<00:00, 31.77it/s]\n",
      "100%|██████████| 500/500 [00:18<00:00, 27.25it/s]\n",
      "100%|██████████| 500/500 [00:22<00:00, 22.21it/s]\n",
      "100%|██████████| 500/500 [00:21<00:00, 23.24it/s]\n",
      "100%|██████████| 500/500 [00:24<00:00, 20.59it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 100.06it/s]\n",
      "100%|██████████| 500/500 [00:04<00:00, 110.92it/s]\n",
      "100%|██████████| 500/500 [00:05<00:00, 92.63it/s] \n",
      "100%|██████████| 500/500 [00:05<00:00, 96.59it/s] \n",
      "100%|██████████| 500/500 [00:04<00:00, 103.70it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 81.31it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 77.97it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 82.85it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 75.17it/s]\n",
      "100%|██████████| 500/500 [00:10<00:00, 47.67it/s]\n",
      "100%|██████████| 500/500 [00:09<00:00, 51.26it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 40.34it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 39.07it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 41.14it/s]\n",
      "100%|██████████| 500/500 [00:13<00:00, 36.65it/s]\n",
      "100%|██████████| 500/500 [00:16<00:00, 31.11it/s]\n",
      "100%|██████████| 500/500 [00:15<00:00, 32.10it/s]\n",
      "100%|██████████| 500/500 [00:18<00:00, 27.37it/s]\n",
      "100%|██████████| 500/500 [00:22<00:00, 22.44it/s]\n",
      "100%|██████████| 500/500 [00:21<00:00, 23.25it/s]\n",
      "100%|██████████| 500/500 [00:24<00:00, 20.73it/s]\n"
     ]
    }
   ],
   "source": [
    "train_args['n_epoch'] = n_epoch\n",
    "# n_tpr = [.05, .1, .15, .2, .25]\n",
    "n_tpr = [25, 50, 100, 150, 200, 300]\n",
    "results = {}\n",
    "batch_size = 1024\n",
    "alpha = 1\n",
    "plot = False\n",
    "use_best_model = True\n",
    "\n",
    "d_phenos = [256]\n",
    "d_embeds = [128]\n",
    "d_recover_embeds = [128]\n",
    "\n",
    "# for seed in [123, 847, 618, 748, 808, 210, 107, 266, 848, 540]:\n",
    "# for seed in [123, 847, 618]:\n",
    "for seed in [748, 808, 210]: # random seeds from an RNG\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    np_rng = np.random.default_rng(seed)\n",
    "    torch_rng = torch.Generator().manual_seed(seed)\n",
    "    pheno_pert_data_shuffle = pheno_pert_data.sample(frac = 1, random_state = np_rng)\n",
    "    \n",
    "    for frac, d_both_embed in train_both_embed.items():\n",
    "        data = split_data(pheno_pert_data_shuffle, n_train = frac, n_val = 0.1, n_test = 0.1)\n",
    "        dataloaders = prepare_dataloaders(data, batch_size, all_genes)\n",
    "        \n",
    "        # Train / evaluate linear model\n",
    "        linear_model = LinearRegression()\n",
    "        train_features = data['train'][['id1_score', 'id2_score']]\n",
    "        train_targets = data['train']['comb_score']\n",
    "        linear_model.fit(train_features, train_targets)\n",
    "\n",
    "        linear_results = {}\n",
    "        linear_features = {}\n",
    "        linear_targets = {}\n",
    "        linear_preds = {}\n",
    "\n",
    "        for split in data:\n",
    "            linear_features[split] = data[split][['id1_score', 'id2_score']]\n",
    "            linear_targets[split] = data[split]['comb_score']\n",
    "            linear_preds[split] = linear_model.predict(linear_features[split])\n",
    "\n",
    "            linear_loss = np.sum((linear_preds[split] - linear_targets[split])**2) / len(linear_preds[split])\n",
    "            linear_corr = np.corrcoef(linear_preds[split], linear_targets[split])[0][1]\n",
    "            linear_spearman = scipy.stats.spearmanr(linear_preds[split], linear_targets[split])\n",
    "            linear_results[split] = {'loss': linear_loss,\n",
    "                                     'corr': linear_corr}\n",
    "            for tpr in n_tpr:\n",
    "                linear_results[split][f'tpr{tpr}'] = calculate_tpr(linear_targets[split], linear_preds[split], tpr)\n",
    "\n",
    "        if plot:\n",
    "            fig = generate_plot([linear_results['train']['loss']],\n",
    "                                [linear_results['val']['loss']],\n",
    "                                linear_targets['train'], linear_targets['val'],\n",
    "                                linear_preds['train'], linear_preds['val'],\n",
    "                                alpha=alpha)\n",
    "            fig.suptitle(f'Model: Linear, Training Fraction: {frac}', fontsize=16)\n",
    "            fig.tight_layout()\n",
    "            fig.show()\n",
    "\n",
    "        results = collect_results(linear_results, n_tpr)\n",
    "        results.update({'model_type': 'linear',\n",
    "                        'd_embed': [-1], \n",
    "                        'd_pheno': [-1],\n",
    "                        'n_train': [frac], \n",
    "                        'seed': [seed]})\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        combined_results = pd.concat([combined_results, results_df], axis = 0, ignore_index = True)\n",
    "\n",
    "        # Train / evaluate SVM model\n",
    "        svr_model = SVR(epsilon=0.01)\n",
    "        train_features = data['train'][['id1_score', 'id2_score']]\n",
    "        train_targets = data['train']['comb_score']\n",
    "        svr_model.fit(train_features, train_targets)\n",
    "\n",
    "        svr_results = {}\n",
    "        svr_features = {}\n",
    "        svr_targets = {}\n",
    "        svr_preds = {}\n",
    "        for split in data:\n",
    "            svr_features[split] = data[split][['id1_score', 'id2_score']]\n",
    "            svr_targets[split] = data[split]['comb_score']\n",
    "            svr_preds[split] = svr_model.predict(svr_features[split])\n",
    "\n",
    "            svr_loss = np.sum((svr_preds[split] - svr_targets[split])**2) / len(svr_preds[split])\n",
    "            svr_corr =  np.corrcoef(svr_preds[split], svr_targets[split])[0][1]\n",
    "            svr_results[split] = {'loss': svr_loss,\n",
    "                                  'corr': svr_corr}\n",
    "            for tpr in n_tpr:\n",
    "                svr_results[split][f'tpr{tpr}'] = calculate_tpr(svr_targets[split], svr_preds[split], tpr)\n",
    "\n",
    "        if plot:\n",
    "            fig = generate_plot([svr_results['train']['loss']], \n",
    "                                [svr_results['val']['loss']], \n",
    "                                svr_targets['train'], svr_targets['val'], \n",
    "                                svr_preds['train'], svr_preds['val'], \n",
    "                                alpha=alpha)\n",
    "            fig.suptitle(f'Model: SVR, Training Fraction: {frac}', fontsize=16)\n",
    "            fig.tight_layout()\n",
    "            fig.show()\n",
    "\n",
    "        results = collect_results(svr_results, n_tpr)\n",
    "        results.update({'model_type': 'svm',\n",
    "                        'd_embed': [-1], \n",
    "                        'd_pheno': [-1],\n",
    "                        'n_train': [frac], \n",
    "                        'seed': [seed]})\n",
    "        results_df = pd.DataFrame(results)\n",
    "        combined_results = pd.concat([combined_results, results_df], axis = 0, ignore_index = True)\n",
    "        \n",
    "        # Train / evaluate combined model\n",
    "        both_model_args = {'model_type': 'both',\n",
    "                           'd_embed': d_both_embed,\n",
    "                           'd_pheno_hid': d_both_pheno,\n",
    "                           'p_dropout': 0.1}\n",
    "        \n",
    "        model, optimizer, lr_scheduler, loss_fn = setup_training(all_genes, dataloaders['train'], both_model_args, train_args)\n",
    "        model, best_model, all_train_loss, all_val_loss = train_model(model, dataloaders, optimizer, lr_scheduler, loss_fn, n_epoch)\n",
    "        if use_best_model:\n",
    "            model_use = best_model\n",
    "        else:\n",
    "            model_use = model\n",
    "        preds = generate_predictions(model_use, dataloaders)\n",
    "        targets = {split: dataloaders[split].dataset.data['comb_score'] for split in dataloaders}\n",
    "        if plot:\n",
    "            fig = generate_plot(all_train_loss, all_val_loss, targets['train'], targets['val'], preds['train'], preds['val'], alpha=alpha)\n",
    "            fig.suptitle(f'Model: Combined, Training Fraction: {frac}', fontsize=16)\n",
    "            fig.tight_layout()\n",
    "            fig.show()\n",
    "    \n",
    "        both_results = {}\n",
    "        both_results['train'] = {'loss': np.mean(all_train_loss[-100:])}\n",
    "        both_results['val'] = {'loss': min(all_val_loss).numpy().item()}\n",
    "        both_results['test'] = {'loss': np.sum((preds['test'] - targets['test'])**2) / dataloaders['test'].dataset.data.shape[0]}\n",
    "        \n",
    "        for split in targets:\n",
    "            both_corr = np.corrcoef(targets[split], preds[split])[0][1]\n",
    "            both_results[split]['corr'] = both_corr\n",
    "\n",
    "            for tpr in n_tpr:\n",
    "                both_results[split][f'tpr{tpr}'] = calculate_tpr(targets[split], preds[split], tpr)\n",
    "\n",
    "        results = collect_results(both_results, n_tpr)\n",
    "        results.update({'model_type': 'both', \n",
    "                        'd_embed': [d_both_embed], \n",
    "                        'd_pheno': [d_both_pheno], \n",
    "                        'n_train': [frac], \n",
    "                        'seed': [seed]})\n",
    "        results_df = pd.DataFrame(results)\n",
    "        combined_results = pd.concat([combined_results, results_df], axis = 0, ignore_index = True)\n",
    "        \n",
    "        # Train / evaluate embedding model\n",
    "        for d_embed in d_embeds:\n",
    "            embed_model_args = {'model_type': 'embed',\n",
    "                                'd_embed': d_embed,\n",
    "                                'p_dropout': 0.1}\n",
    "            \n",
    "            model, optimizer, lr_scheduler, loss_fn = setup_training(all_genes, dataloaders['train'], embed_model_args, train_args)\n",
    "            model, best_model, all_train_loss, all_val_loss = train_model(model, dataloaders, optimizer, lr_scheduler, loss_fn, n_epoch)\n",
    "            if use_best_model:\n",
    "                model_use = best_model\n",
    "            else:\n",
    "                model_use = model\n",
    "            preds = generate_predictions(model_use, dataloaders)\n",
    "            targets = {split: dataloaders[split].dataset.data['comb_score'] for split in dataloaders}\n",
    "            if plot:\n",
    "                fig = generate_plot(all_train_loss, all_val_loss, targets['train'], targets['val'], preds['train'], preds['val'], alpha=alpha)\n",
    "                fig.suptitle(f'Model: Embed, Training Fraction: {frac}', fontsize=16)\n",
    "                fig.tight_layout()\n",
    "                fig.show()\n",
    "\n",
    "            embed_results = {}\n",
    "            embed_results['train'] = {'loss': np.mean(all_train_loss[-100:])}\n",
    "            embed_results['val'] = {'loss': min(all_val_loss).numpy().item()}\n",
    "            embed_results['test'] = {'loss': np.sum((preds['test'] - targets['test'])**2) / dataloaders['test'].dataset.data.shape[0]}\n",
    "            \n",
    "            for split in targets:\n",
    "                embed_corr = np.corrcoef(targets[split], preds[split])[0][1]\n",
    "                embed_results[split]['corr'] = embed_corr\n",
    "\n",
    "                for tpr in n_tpr:\n",
    "                    embed_results[split][f'tpr{tpr}'] = calculate_tpr(targets[split], preds[split], tpr)\n",
    "\n",
    "            results = collect_results(embed_results, n_tpr)\n",
    "            results.update({'model_type': 'embed', \n",
    "                            'd_embed': [d_embed], \n",
    "                            'd_pheno': [-1], \n",
    "                            'n_train': [frac], \n",
    "                            'seed': [seed]})\n",
    "            results_df = pd.DataFrame(results)\n",
    "            combined_results = pd.concat([combined_results, results_df], axis = 0, ignore_index = True)\n",
    "        \n",
    "        # Train / evaluate RECOVER model\n",
    "        for d_embed in d_recover_embeds:\n",
    "            recover_model_args = {'model_type': 'recover',\n",
    "                                  'd_embed': d_embed,\n",
    "                                  'p_dropout': 0.1}\n",
    "            recover_train_args = {'embed_lr': 1e-3,\n",
    "                                  'weight_decay': 1e-4,\n",
    "                                  'n_epoch': n_epoch}\n",
    "            \n",
    "            model, optimizer, lr_scheduler, loss_fn = setup_training(all_genes, dataloaders['train'], recover_model_args, recover_train_args)\n",
    "            model, best_model, all_train_loss, all_val_loss = train_model(model, dataloaders, optimizer, lr_scheduler, loss_fn, n_epoch)\n",
    "            if use_best_model:\n",
    "                model_use = best_model\n",
    "            else:\n",
    "                model_use = model\n",
    "            preds = generate_predictions(model_use, dataloaders)\n",
    "            targets = {split: dataloaders[split].dataset.data['comb_score'] for split in dataloaders}\n",
    "            if plot:\n",
    "                fig = generate_plot(all_train_loss, all_val_loss, targets['train'], targets['val'], preds['train'], preds['val'], alpha=alpha)\n",
    "                fig.suptitle(f'Model: Recover, Training Fraction: {frac}', fontsize=16)\n",
    "                fig.tight_layout()\n",
    "                fig.show()\n",
    "\n",
    "            recover_results = {}\n",
    "            recover_results['train'] = {'loss': np.mean(all_train_loss[-100:])}\n",
    "            recover_results['val'] = {'loss': min(all_val_loss).numpy().item()}\n",
    "            recover_results['test'] = {'loss': np.sum((preds['test'] - targets['test'])**2) / dataloaders['test'].dataset.data.shape[0]}\n",
    "            \n",
    "            for split in targets:\n",
    "                recover_corr = np.corrcoef(targets[split], preds[split])[0][1]\n",
    "                recover_results[split]['corr'] = recover_corr\n",
    "\n",
    "                for tpr in n_tpr:\n",
    "                    recover_results[split][f'tpr{tpr}'] = calculate_tpr(targets[split], preds[split], tpr)\n",
    "\n",
    "            results = collect_results(recover_results, n_tpr)\n",
    "            results.update({'model_type': 'recover', \n",
    "                            'd_embed': [d_embed], \n",
    "                            'd_pheno': [-1], \n",
    "                            'n_train': [frac], \n",
    "                            'seed': [seed]})\n",
    "            results_df = pd.DataFrame(results)\n",
    "            combined_results = pd.concat([combined_results, results_df], axis = 0, ignore_index = True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_results_file = f'./results/downsample_{data_source}_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results.to_csv(downsample_results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gears_result_dir = f'./gears/results/'\n",
    "\n",
    "gears_results = []\n",
    "for f in os.listdir(gears_result_dir):\n",
    "    if data_source in f:\n",
    "        re_match = re.search(r'seed(\\d+)', f)\n",
    "        seed = re_match.group(1)\n",
    "        re_match = re.search(r'nsample(\\d+)', f)\n",
    "        n_sample = re_match.group(1)\n",
    "        results = pd.read_csv(gears_result_dir + f)\n",
    "        mse = np.mean((results['pred'] - results['truth'])**2)\n",
    "        corr = np.corrcoef(results['truth'], results['pred'])[0][1]\n",
    "        # spearman = spearmanr(results['truth'], results['pred'])\n",
    "        result_dict = {'model_type': 'gears',\n",
    "                        'seed': seed, \n",
    "                        'n_train': int(n_sample),\n",
    "                        'test_loss': mse,\n",
    "                        'test_corr': corr}\n",
    "        \n",
    "        for n_tpr in [25, 50, 100, 150, 200, 300]:\n",
    "            tpr = calculate_tpr(results['truth'], results['pred'], n_tpr)\n",
    "            result_dict[f'test_tpr{n_tpr}'] = tpr\n",
    "        \n",
    "        gears_results.append(result_dict)\n",
    "gears_results = pd.DataFrame(gears_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(downsample_results_file, index_col=0)\n",
    "df = pd.concat([df, gears_results], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_benchmark = {}\n",
    "if 'train_frac' in df.columns:\n",
    "    df = df.rename(columns={'train_frac': 'n_train'})\n",
    "performance_benchmark = df.groupby(['n_train', 'model_type']).agg(['mean', 'std'])[['test_loss', 'test_corr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_benchmark = performance_benchmark.reset_index()\n",
    "performance_benchmark['gene_frequency'] = performance_benchmark['n_train'] / n_gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_benchmark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naiad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

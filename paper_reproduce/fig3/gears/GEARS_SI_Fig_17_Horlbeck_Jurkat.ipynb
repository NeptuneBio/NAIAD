{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2024a152",
   "metadata": {},
   "source": [
    "### NOTE: must set up GEARS via this repo to run this notebook: https://github.com/yhr91/GEARS_misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "sys.path.append('../../GEARS_misc/') # add GEARS_misc to path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from gears import PertData, GEARS\n",
    "\n",
    "results_folder = './results/'\n",
    "dataset = 'jurkat'\n",
    "data_path = './'\n",
    "model = 'gears'\n",
    "device = 'cuda'\n",
    "seed = 10\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def load_data(seed):\n",
    "    pert_data = PertData(data_path, gi_go=True) # specific saved folder\n",
    "    # pert_data = PertData(data_path, gene_path='../../data/gears_reproduce/essential_norman.pkl')\n",
    "    pert_data.load(data_path= data_path+dataset) # load the processed data, the path is saved folder + dataset_name\n",
    "    # pert_data.load(data_name = 'norman') # load the processed data, the path is saved folder + dataset_name\n",
    "    pert_data.prepare_split(split = 'simulation', seed = seed)\n",
    "    pert_data.get_dataloader(batch_size = 256, test_batch_size = 256)\n",
    "    \n",
    "    adata_df = pert_data.adata.to_df()\n",
    "    adata_df['condition'] = pert_data.adata.obs['condition']\n",
    "    mean_df = adata_df.groupby('condition').mean()\n",
    "    ctrl_mean = mean_df.loc['ctrl']\n",
    "    \n",
    "    return pert_data, ctrl_mean, mean_df\n",
    "\n",
    "def load_model(pert_data):\n",
    "    gears_model = GEARS(pert_data, device = 'cuda', \n",
    "                weight_bias_track = False, \n",
    "                proj_name = 'gears', \n",
    "                exp_name = 'gears',\n",
    "                gi_predict = True)\n",
    "    gears_model.model_initialize(hidden_size = 256)\n",
    "    \n",
    "    return gears_model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These perturbations are not in the GO graph and is thus not able to make prediction for...\n",
      "['AARS2+ADPRM' 'AARS2+ASNA1' 'AARS2+ATP5F1' ... 'USMG5+ZNRD1'\n",
      " 'USMG5+ZWINT' 'USMG5+ctrl']\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:5460\n",
      "combo_seen1:32240\n",
      "combo_seen2:12052\n",
      "unseen_single:104\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pert_data, _, _ = load_data(seed)\n",
    "gears_model = load_model(pert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "095b8d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>condition</th>\n",
       "      <th>dose_val</th>\n",
       "      <th>control</th>\n",
       "      <th>condition_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARS2+AARS2</td>\n",
       "      <td>1+1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jurkat_AARS2+AARS2_1+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARS2+AATF</td>\n",
       "      <td>1+1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jurkat_AARS2+AATF_1+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AARS2+ABCB7</td>\n",
       "      <td>1+1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jurkat_AARS2+ABCB7_1+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AARS2+ACTL6A</td>\n",
       "      <td>1+1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jurkat_AARS2+ACTL6A_1+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AARS2+ACTR10</td>\n",
       "      <td>1+1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jurkat_AARS2+ACTR10_1+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96236</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>1+1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jurkat_ctrl_1+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96237</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>1+1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jurkat_ctrl_1+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96238</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>1+1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jurkat_ctrl_1+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96239</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>1+1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jurkat_ctrl_1+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96240</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>1+1</td>\n",
       "      <td>1</td>\n",
       "      <td>Jurkat_ctrl_1+1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86419 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          condition dose_val control           condition_name\n",
       "0       AARS2+AARS2      1+1       1   Jurkat_AARS2+AARS2_1+1\n",
       "1        AARS2+AATF      1+1       1    Jurkat_AARS2+AATF_1+1\n",
       "2       AARS2+ABCB7      1+1       1   Jurkat_AARS2+ABCB7_1+1\n",
       "3      AARS2+ACTL6A      1+1       1  Jurkat_AARS2+ACTL6A_1+1\n",
       "4      AARS2+ACTR10      1+1       1  Jurkat_AARS2+ACTR10_1+1\n",
       "...             ...      ...     ...                      ...\n",
       "96236          ctrl      1+1       1          Jurkat_ctrl_1+1\n",
       "96237          ctrl      1+1       1          Jurkat_ctrl_1+1\n",
       "96238          ctrl      1+1       1          Jurkat_ctrl_1+1\n",
       "96239          ctrl      1+1       1          Jurkat_ctrl_1+1\n",
       "96240          ctrl      1+1       1          Jurkat_ctrl_1+1\n",
       "\n",
       "[86419 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert_data.adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These perturbations are not in the GO graph and is thus not able to make prediction for...\n",
      "['AARS2+ADPRM' 'AARS2+ASNA1' 'AARS2+ATP5F1' ... 'USMG5+ZNRD1'\n",
      " 'USMG5+ZWINT' 'USMG5+ctrl']\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Simulation split test composition:\n",
      "combo_seen0:5460\n",
      "combo_seen1:32240\n",
      "combo_seen2:12052\n",
      "unseen_single:104\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pert_data, ctrl_mean, mean_df = load_data(seed)\n",
    "gears_model = load_model(pert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### turn the model on \n",
    "model = gears_model.model\n",
    "model.cell_fitness_pred = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = gears_model.adata\n",
    "ctrl_adata = adata[adata.obs['condition'] == 'ctrl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9853"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gears_model.pert_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pert_list_options = gears_model.pert_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'../../data/horlbeck/{dataset}/GI_data_horlbeck_{dataset}.pkl', 'rb') as f:\n",
    "    fitness_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['good_phen', 'fitness_mapper_gene', 'good_genes'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pert_genes = fitness_data['good_phen'].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "pert_list = [i[0] + '+' + i[1] for i in list(combinations(fitness_data['good_phen'].index.values, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(i):\n",
    "    if i.split('_')[0] == i.split('_')[1]:\n",
    "        return i.split('_')[0]\n",
    "    #elif 'neg' in i:\n",
    "    #    print(i)\n",
    "    else:\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "class GI_expt():\n",
    "    # TODO get rid of hold out genes\n",
    "    def __init__(self, raw=False, pick_genes = None, set_train = None):\n",
    "        # sample: matrix sampling percentage\n",
    "        # seed: sampling seed,\n",
    "        # sampled_data: sampled indices from upper traingle matrix\n",
    "        # test: matrix values to be predicted\n",
    "        # train: observed matrix values used for training, \n",
    "        # delta: deviation from expectation as inferred using \n",
    "        #        a regression model trained on sampled_data (corresponds to sampled_data)\n",
    "        # delta_tot: delta values computed for all matrix values\n",
    "        # y_tot: Full true matrix (raw fitness values)\n",
    "        # raws: Sampled raw fitness values (corresponds to sampled_data)\n",
    "        # transformer: regression model used to determine expected fitness values\n",
    "        \n",
    "\n",
    "        # with open('/dfs/project/perturb-gnn/datasets/Horlbeck2018/GI_data_horlbeck_jurkat.pkl', 'rb') as f:\n",
    "        # with open('../../data/horlbeck/jurkat/GI_data_horlbeck_jurkat.pkl', 'rb') as f:\n",
    "        with open(f'../../data/horlbeck/{dataset}/GI_data_horlbeck_{dataset}.pkl', 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "            \n",
    "        self.data['fitness_mapper_gene'] = {convert(i): j for i,j in self.data['fitness_mapper_gene'].items()}\n",
    "        self.raw = raw\n",
    "        self.pick_genes = pick_genes\n",
    "        self.set_train = set_train\n",
    "\n",
    "    def pre_process(self, sample, seed):\n",
    "        itr_data = {}\n",
    "        itr_data['sample'] = sample\n",
    "        itr_data['seed'] = seed\n",
    "\n",
    "        # Randomly sample from the fitness matrix\n",
    "        if self.set_train is not None:\n",
    "            itr_data['sampled_data'] = self.set_train\n",
    "        elif self.pick_genes is None:\n",
    "            itr_data['sampled_data'] = \\\n",
    "               upper_triangle(self.data['good_phen'], k=0).sample(\n",
    "               frac=float(sample) / 100., replace=False, random_state=seed).index\n",
    "        else:\n",
    "            itr_data['sampled_data'] = \\\n",
    "                upper_triangle(self.data['good_phen'].iloc[self.pick_genes,\n",
    "                                                    self.pick_genes]).index\n",
    "\n",
    "        # Set up masked delta matrix for performing matrix completion\n",
    "        itr_data['delta'], itr_data['test'], \\\n",
    "        itr_data['train'], itr_data['delta_tot'], \\\n",
    "        itr_data['X_tot'], itr_data['y_tot'], itr_data['transformer'], \\\n",
    "        itr_data['raws'] = \\\n",
    "            get_masked_delta_matrix(self.data['good_phen'],\n",
    "                                    itr_data['sampled_data'],\n",
    "                                    self.data['fitness_mapper_gene'],\n",
    "                                    self.data['good_genes'])\n",
    "\n",
    "        return itr_data\n",
    "\n",
    "\n",
    "def get_masked_delta_matrix(data, sampling, phen_mapper, good_genes):\n",
    "    masked_data, mask = get_masked_data(data, sampling)\n",
    "\n",
    "    upper_masked_data = upper_triangle(masked_data)\n",
    "    predicted = upper_masked_data[upper_masked_data == 0].index\n",
    "    given = upper_masked_data[upper_masked_data != 0].index\n",
    "\n",
    "    delta_tot, X, X_tot, y, y_tot, transformer, raws = get_deltas(data, masked_data, phen_mapper, good_genes)\n",
    "    delta, _ = get_masked_data(delta_tot, sampling)\n",
    "    raws, _ = get_masked_data(raws, sampling)\n",
    "\n",
    "    return delta, predicted, given, delta_tot, X_tot, y_tot, transformer, raws\n",
    "\n",
    "def get_deltas(source_data, sampled_data, phen_mapper, good_genes):\n",
    "    # this is the routine that fits a quadratic model to observed fitness measurements and returns\n",
    "    # the deviations from the expectation given by that model (the \"deltas\")\n",
    "    y = sampled_data.stack()\n",
    "    y = y[y != 0]\n",
    "\n",
    "    X = np.concatenate([y.index.get_level_values(0).map(lambda x: phen_mapper[x]).values[:, np.newaxis],\n",
    "                        y.index.get_level_values(1).map(lambda x: phen_mapper[x]).values[:, np.newaxis]], axis=1)\n",
    "\n",
    "    y_tot = source_data.loc[good_genes, good_genes].stack()\n",
    "    X_tot = np.concatenate([y_tot.index.get_level_values(0).map(lambda x: phen_mapper[x]).values[:, np.newaxis],\n",
    "                            y_tot.index.get_level_values(1).map(lambda x: phen_mapper[x]).values[:, np.newaxis]],\n",
    "                           axis=1)\n",
    "\n",
    "    transformer = PolynomialFeatures()\n",
    "    model = make_pipeline(PolynomialFeatures(), LinearRegression())\n",
    "    model.fit(X, y)\n",
    "\n",
    "    delta = pd.Series(y_tot - model.predict(X_tot), index=y_tot.index)\n",
    "    raws = pd.Series(y_tot, index=y_tot.index).unstack()\n",
    "    delta = delta.unstack()\n",
    "\n",
    "    return delta, X, X_tot, y, y_tot, model, raws\n",
    "\n",
    "def upper_triangle(M, k=1):\n",
    "    \"\"\" Copyright (C) 2019  Thomas Norman\n",
    "    Return the upper triangular part of a matrix in stacked format (i.e. as a vector)\n",
    "    \"\"\"\n",
    "    keep = np.triu(np.ones(M.shape), k=k).astype('bool').reshape(M.size)\n",
    "    return M.stack(dropna=False).loc[keep]\n",
    "\n",
    "def get_masked_data(df, ind, mean_normalize=False):\n",
    "    masked_data = df.copy().values\n",
    "    mask = pd.DataFrame(0, index=df.index, columns=df.columns)\n",
    "\n",
    "    for gene1, gene2 in ind:\n",
    "        mask.loc[gene1, gene2] = 1\n",
    "\n",
    "        # assume DataFrame is symmetric\n",
    "    mask = mask + mask.T\n",
    "    mask = (mask != 0).values\n",
    "    masked_data[~mask] = 0\n",
    "    masked_data_df = pd.DataFrame(masked_data, index=df.index, columns=df.columns)\n",
    "\n",
    "    # whether to center the observed entries such that the overall mean is 0\n",
    "    if mean_normalize:\n",
    "        masked_data_df = masked_data_df.stack()\n",
    "        offset = masked_data_df[masked_data_df != 0]\n",
    "        offset = offset.mean()\n",
    "        masked_data_df[masked_data_df != 0] = masked_data_df[masked_data_df != 0] - offset\n",
    "        print(offset)\n",
    "        masked_data_df = masked_data_df.unstack()\n",
    "\n",
    "    return masked_data_df, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [07:54<00:00, 202.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:07<00:00,  5.10it/s]\n",
      "100%|██████████| 6673/6673 [04:21<00:00, 25.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08886251469980647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [07:46<00:00, 206.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:11<00:00,  5.30it/s]\n",
      "100%|██████████| 6645/6645 [04:29<00:00, 24.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17974118719429372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:00<00:00, 199.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:18<00:00,  5.19it/s]\n",
      "100%|██████████| 6610/6610 [04:33<00:00, 24.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04854963337285911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [07:48<00:00, 205.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:30<00:00,  5.09it/s]\n",
      "100%|██████████| 6540/6540 [04:36<00:00, 23.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15253699653033126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:04<00:00, 198.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 202/220 [00:40<00:03,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 201 Train Loss: 0.1325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:43<00:00,  5.03it/s]\n",
      "100%|██████████| 6470/6470 [04:31<00:00, 23.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0422433259099981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [07:51<00:00, 203.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 202/284 [00:40<00:16,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 201 Train Loss: 0.0244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:56<00:00,  5.03it/s]\n",
      "100%|██████████| 6400/6400 [04:29<00:00, 23.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10960124078263292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:17<00:00, 193.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 202/345 [00:40<00:28,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 201 Train Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 345/345 [01:09<00:00,  4.99it/s]\n",
      "100%|██████████| 6333/6333 [04:23<00:00, 24.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06464982658337151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [07:53<00:00, 202.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 202/409 [00:40<00:41,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 201 Train Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 402/409 [01:20<00:01,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 401 Train Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 409/409 [01:21<00:00,  5.01it/s]\n",
      "100%|██████████| 6261/6261 [04:15<00:00, 24.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0856231907167668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:02<00:00, 199.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:06<00:00,  5.45it/s]\n",
      "100%|██████████| 6673/6673 [04:49<00:00, 23.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0039924451936962264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [07:55<00:00, 202.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:12<00:00,  5.19it/s]\n",
      "100%|██████████| 6646/6646 [04:46<00:00, 23.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007531483741503366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:11<00:00, 195.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:18<00:00,  5.17it/s]\n",
      "100%|██████████| 6610/6610 [04:29<00:00, 24.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002746798034427577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:09<00:00, 196.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:31<00:00,  5.05it/s]\n",
      "100%|██████████| 6540/6540 [04:27<00:00, 24.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0032517762077622476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:20<00:00, 192.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 202/221 [00:40<00:03,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 201 Train Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:43<00:00,  5.02it/s]\n",
      "100%|██████████| 6470/6470 [04:28<00:00, 24.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0067860224141509965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:01<00:00, 199.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 202/285 [00:40<00:16,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 201 Train Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 285/285 [00:57<00:00,  4.98it/s]\n",
      "100%|██████████| 6399/6399 [04:31<00:00, 23.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008035637553658881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:09<00:00, 196.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 201/349 [00:40<00:29,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 201 Train Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 349/349 [01:12<00:00,  4.82it/s]\n",
      "100%|██████████| 6328/6328 [04:39<00:00, 22.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0031634432853144987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:01<00:00, 199.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 202/411 [00:40<00:41,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 201 Train Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 402/411 [01:20<00:01,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 401 Train Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 411/411 [01:22<00:00,  4.99it/s]\n",
      "100%|██████████| 6258/6258 [04:27<00:00, 23.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0026965347888413425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:07<00:00, 197.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:06<00:00,  5.46it/s]\n",
      "100%|██████████| 6673/6673 [04:36<00:00, 24.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028402375630020675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:02<00:00, 199.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:12<00:00,  5.24it/s]\n",
      "100%|██████████| 6644/6644 [04:31<00:00, 24.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003987722754647589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:09<00:00, 196.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [00:18<00:00,  5.15it/s]\n",
      "100%|██████████| 6610/6610 [04:32<00:00, 24.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004202736588699826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:06<00:00, 197.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:43<00:00,  3.57it/s]\n",
      "100%|██████████| 6540/6540 [05:42<00:00, 19.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020108390375955354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:27<00:00, 189.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 201/219 [00:57<00:05,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 201 Train Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [01:02<00:00,  3.52it/s]\n",
      "100%|██████████| 6471/6471 [05:25<00:00, 19.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0480812792906808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:08<00:00, 196.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 201/280 [00:56<00:22,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 201 Train Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [01:19<00:00,  3.54it/s]\n",
      "100%|██████████| 6404/6404 [05:26<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011432818195660499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:18<00:00, 192.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 201/341 [00:56<00:39,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 201 Train Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 341/341 [01:36<00:00,  3.54it/s]\n",
      "100%|██████████| 6336/6336 [05:16<00:00, 19.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0050094542671651155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96141/96141 [08:11<00:00, 195.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10236\n",
      "Start Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 201/403 [00:57<00:56,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 201 Train Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 401/403 [01:53<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Step 401 Train Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 403/403 [01:53<00:00,  3.55it/s]\n",
      "100%|██████████| 6265/6265 [05:29<00:00, 19.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015234327622951469\n"
     ]
    }
   ],
   "source": [
    "# Run prediction\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from copy import deepcopy\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_all = []\n",
    "    loss_all = []\n",
    "    pert_all = []\n",
    "    y_true_all = []\n",
    "    for step, batch in enumerate(tqdm(loader)):\n",
    "        batch = batch.to(device)\n",
    "        out, y_pred = model(batch)\n",
    "        loss = loss_fct(batch.y.reshape(-1), y_pred.reshape(-1))\n",
    "        loss_all.append(loss.item())\n",
    "        y_all.append(y_pred.detach().cpu().numpy())\n",
    "        pert_all.append(batch.pert)\n",
    "        y_true_all.append(batch.y)\n",
    "    return y_all, loss_all, pert_all, y_true_all\n",
    "\n",
    "def get_dataloader(set2cond, batch_size, test_batch_size = None):\n",
    "    if test_batch_size is None:\n",
    "        test_batch_size = batch_size\n",
    "\n",
    "    cell_graphs = {}\n",
    "    \n",
    "\n",
    "    splits = ['train','val','test']\n",
    "    for i in splits:\n",
    "        cell_graphs[i] = []\n",
    "        for p in set2cond[i]:\n",
    "            if p in all_cell_graphs:\n",
    "                cell_graphs[i].extend(all_cell_graphs[p])\n",
    "\n",
    "    # Set up dataloaders\n",
    "    train_loader = DataLoader(cell_graphs['train'],\n",
    "                        batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "    val_loader = DataLoader(cell_graphs['val'],\n",
    "                        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = DataLoader(cell_graphs['test'],\n",
    "                    batch_size=batch_size, shuffle=False)\n",
    "    return {'train_loader': train_loader,\n",
    "                        'val_loader': val_loader,\n",
    "                        'test_loader': test_loader}\n",
    "\n",
    "# for seed in [123, 847, 618, 748, 808]:\n",
    "n_train = 96141\n",
    "n_samples = [100, 500, 1000, 2000, 3000, 4000, 5000, 6000]\n",
    "for seed in [123, 847, 618]:\n",
    "    for n_sample in n_samples:\n",
    "        for label in ['raw_y']:\n",
    "            sample_frac = float(n_sample) / float(n_train) * 100\n",
    "\n",
    "            GI = GI_expt(pick_genes=None)\n",
    "            itr_data = GI.pre_process(sample=sample_frac, seed=seed)\n",
    "\n",
    "            itr_data['raw_y'] = fitness_data['good_phen']\n",
    "            y = itr_data[label].values\n",
    "\n",
    "            gene2idx = dict(zip(itr_data[label].index.values, range(len(itr_data[label].index.values))))\n",
    "\n",
    "            pert_list = [i[0] + '+' + i[1] for i in list(combinations(itr_data[label].index.values, 2))]\n",
    "            pert_list += [i + '+ctrl' for i in unique_pert_genes]\n",
    "\n",
    "            y_list = []\n",
    "            for i in pert_list:\n",
    "                if i.split('+')[1]!= 'ctrl':\n",
    "                    y_list.append(y[gene2idx[i.split('+')[0]], gene2idx[i.split('+')[1]]])\n",
    "                else:\n",
    "                    y_list.append(y[gene2idx[i.split('+')[0]], gene2idx[i.split('+')[0]]])\n",
    "\n",
    "            pert_list2y_list = dict(zip(pert_list, y_list))\n",
    "\n",
    "            train = [i[0] + '+' + i[1] for i in itr_data['train']]\n",
    "            test = [i[0] + '+' + i[1] for i in itr_data['test']]\n",
    "\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            train, val, _, _ = train_test_split(train, [-1] * len(train), test_size=0.1, random_state=seed)\n",
    "\n",
    "            ## add single back\n",
    "            train = train + [i + '+ctrl' for i in unique_pert_genes]\n",
    "\n",
    "            set2cond = {\n",
    "                'train': train,\n",
    "                'val': val,\n",
    "                'test': test\n",
    "            }\n",
    "\n",
    "            from torch_geometric.data import Data\n",
    "            import torch\n",
    "            from tqdm import tqdm\n",
    "            num_samples = 20\n",
    "            all_cell_graphs = {}\n",
    "            pert_na = []\n",
    "            for pert in tqdm(pert_list):\n",
    "                pert_name = pert\n",
    "                pert = pert.split('+')\n",
    "                pert = [i for i in pert if i!='ctrl']\n",
    "\n",
    "                # Get the indices (and signs) of applied perturbation\n",
    "                try:\n",
    "                    pert_idx = [np.where(p == np.array(all_pert_list_options))[0][0] for p in pert]\n",
    "                    Xs = ctrl_adata[np.random.randint(0, len(ctrl_adata), num_samples), :].X.toarray()\n",
    "                    # Create cell graphs\n",
    "                    all_cell_graphs[pert_name] = [Data(x=torch.Tensor(X).T, pert_idx = pert_idx, pert=pert, y=pert_list2y_list[pert_name]) for X in Xs]\n",
    "                except:\n",
    "                    pert_na.append(pert_name)\n",
    "\n",
    "            print(len(pert_na))\n",
    "            loaders = get_dataloader(set2cond, batch_size = 256)\n",
    "            train_loader = loaders['train_loader']\n",
    "            val_loader = loaders['val_loader']\n",
    "            test_loader = loaders['test_loader']\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay = 5e-4)\n",
    "            scheduler = StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "\n",
    "            loss_fct = F.mse_loss\n",
    "            earlystop_validation_metric = 'mse'\n",
    "            binary_output = False\n",
    "\n",
    "            earlystop_direction = 'descend'\n",
    "            min_val = np.inf\n",
    "\n",
    "            best_model = deepcopy(model).to(device)\n",
    "\n",
    "            loss_history = {\n",
    "                'loss': []\n",
    "            }\n",
    "\n",
    "            print('Start Training...')\n",
    "            for epoch in range(1):\n",
    "                model.train()\n",
    "\n",
    "                for step, batch in enumerate(tqdm(train_loader)):\n",
    "                    optimizer.zero_grad()\n",
    "                    batch = batch.to(device)\n",
    "                    out, y_pred = model(batch)\n",
    "                    loss = loss_fct(batch.y.float().reshape(-1), y_pred.reshape(-1))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    loss_history['loss'].append(loss.item())\n",
    "\n",
    "                    if (step % 200 == 0) and (step >= 200):\n",
    "                        log = \"Epoch {} Step {} Train Loss: {:.4f}\" \n",
    "                        print(log.format(epoch + 1, step + 1, loss.item()))\n",
    "                #_, val_loss\n",
    "                # val_y_all, val_loss_all, val_pert_all, val_y_true_all = evaluate(model, val_loader)\n",
    "                # if np.mean(val_loss_all) < min_val:\n",
    "                #     best_model = deepcopy(model)\n",
    "                #     min_val = np.mean(val_loss_all)\n",
    "                best_model = deepcopy(model)\n",
    "            test_y_all, test_loss_all, test_pert_all, test_y_true_all = evaluate(best_model, test_loader)\n",
    "            print(np.mean(test_loss_all))\n",
    "\n",
    "\n",
    "            pert_test_list = [i[0] + '+' +  i[1] if len(i) == 2 else i[0] +'+ctrl' for i in [j for i in test_pert_all for j in i]]\n",
    "            test_y_list = np.concatenate([i for i in [j for i in test_y_all for j in i]])\n",
    "            test_y_true_list = np.concatenate([i.detach().cpu().numpy() for i in test_y_true_all])\n",
    "            df_pred = pd.DataFrame((pert_test_list, test_y_true_list, test_y_list)).T\n",
    "            pert2y_true = dict(df_pred.groupby([0])[1].agg(np.mean))\n",
    "            pert2y_pred = dict(df_pred.groupby([0])[2].agg(np.mean))\n",
    "\n",
    "            pred = pd.DataFrame(pert2y_true, index = [0]).T.reset_index().rename(columns = {0: 'truth', 'index': 'test_pert'})\n",
    "            pred['pred'] = pred.test_pert.apply(lambda x: pert2y_pred[x])\n",
    "            pred.to_csv(results_folder + label + '_pred_no_pretrain_seed' + str(seed) + '_nsample' + str(n_sample) + dataset + '_gi.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gears-old-gh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
